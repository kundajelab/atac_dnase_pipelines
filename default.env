## Get hostname with the following command: 
## $ hostname -f
##
## Configure environment per hostname:
## [hostname1]
## ...
##
## Use the same environment for multiple hostnames:
## [hostname2, hostname3, ...]
## ...
##
## Using group
## [hostname1, hostname2, ... : group]
## [group]
## ...
##
## Using an asterisk in hostnames (IMPORTANT: only one * is allowed in hostnames)
##
## [host*name1]
##
## [*hostname2, hostname3*]


# Stanford Kundaje group clusters (out of SGE)
[vayu, mitra, durga]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= $script_dir/species/kundaje.conf
unlimited_mem_wt= true 		# unlimited max. memory and walltime on Kundaje clusters
nice 		= 10
nth 		= 4

# Stanford Kundaje group clusters (controlled with SGE)
[nandi, kali, amold, wotan, kadru, surya]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= $script_dir/species/kundaje.conf
unlimited_mem_wt= true 		# unlimited max. memory and walltime on Kundaje clusters
system 		= sge
nice 		= 10
nth 		= 4

# Stanford SCG4
[scg*.stanford.edu, scg*.local, carmack.stanford.edu, crick.stanford.edu]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= $script_dir/species/scg.conf
nth		= 8		# number of threads for each pipeline
system		= sge		# force to use SGE (Sun Grid Engine) on SCG3/4 even though a user doesn't explicitly specify SGE on command line with 'bds -s sge atac.bds ...'
retrial		= 1		# number of retrial for failed tasks (SCG queue master kills tasks very frequently according to node's available resources)


# Stanford Sherlock clusters
[sherlock*.stanford.edu, sh-*.local]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= $script_dir/species/sherlock.conf
nth		= 8		# number of threads for each pipeline
system		= slurm
retrial		= 1		# number of retrial for failed tasks


# default
[default]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= # use your own species file here. (DEF_SPECIES_FILE: DO NOT REMOVE THIS COMMENT!)

